1. Introduction
  AI is slowly replaced human in some daily tasks nowadays. From housework, transportation and even making decision of someone's life. It is more concerning now that technologies are capable of choosing and engaging targets without human control. Because of that, many questions are raised about this problem, but the most asked question is whether AI systems should be permitted to exercise lethal force. Although AI is very convenient and brings many advantages for work, the rules or the mandatory guardrails for AI have not been finalized and failed to meet the safety requirements to be able to use it.
  The most important thing in these systems is not about how they are developed, but a profound ethical challenge. Eventhough AI is every smart and fast, it still lacks of human emotion, empathy and moral judgment. This report will argue that weapons run on its own should not be allowed to exercise lethal force. To understand why these systems are too dangerous, we should take a clearer look at the ethical responsibility of the people who build these robots and how the Australian government made these safety rules.

2. An overview of documents D1 and D2
  To understand the danger of lethal AI we should look at how the technology is created in the first place. In D1, Burton and other researchers argue that ethical considerations in AI should not be the final check at the end of the development, they should be integrated into the very first stages of an AI development. Normally, developers only focus on how to make an AI work fast and accurately and they treat ethical safety as an afterthought. Burton argues this is a huge mistake because if we want the machine or the AI to be safe to use, developers need to be taught to integrate ethics in the very first steps of the development. Therefore, they suggest that the responsibility for the machine's behavior is strictly linked to the building process. If not, it means that the engineer is not taught to prioritize ethical constrains, they might build a system that only focus on completing the mission, ignoring the value of a human life. If we build a lethal weapon that runs independently without proactive ethics, the product has already failed right in the lab before it even reaches the battlefield. Therefore, the people coding these weapons must carry some of the moral weight for what the machine eventually does.
  Moving to Document D2, while the Burton and researchers focus on the builders in D1, goverments must follow the laws. The Australian Government has recently addressed this in a 2024 proposals paper about safe and responsible AI. They see that AI can be useful but can also be very dangerous if not treat properlly. To prevent this from harming humans, the Australian Government is proposing "mandatory guardrails". These are strict, unbreakable safety rules for any AI used in such dangerous areas. A "high-risk setting" is defined as any situation where AI could hurt someone's physical safety, mental health, or legal rights. A weapon designed to kill people is the ultimate high-risk setting. According to the government, high-risk AI must follow specific rules. First, there must be accountability, meaning it is clear who is legally responsible if things go wrong. Second, there must be transparency, meaning people can understand how the AI made its decision. Finally, there must be human control, meaning a person must be able to oversee and step in to stop the machine.
  When we combine the ethical warnings from developers (D1) with the strict legal safety rules from the government (D2), a clear problem emerges. Lethal Autonomous Weapons Systems are designed to operate without human control. By their very nature, they break the mandatory guardrails that the Australian Government says are necessary for public safety.
  To prove that AI should not be allowed to kill, the rest of this report will explore three key problems in detail. First, it will examine the Accountability Gap, looking at why it is legally impossible to punish a machine if it commits a war crime. Second, it will explore the Transparency Problem, explaining why modern AI is often a "black box" that even its creators cannot fully understand. Finally, it will discuss the Failure of Meaningful Human Control, showing that when machines move at lightning speed, human oversight becomes nothing more than an illusion.

Section 1: The Accountability Gap
  In traditional warfare, there is a clear chain of command. If a soldier makes a terrible mistake and kills an innocent civilian, that soldier—or their commanding officer—can be put on trial and sent to prison. This is how international law works, a human is always held responsible. However, when we replace human soldiers with Artificial Intelligence (AI), this clear line of responsibility disappears. This problem is known as the Accountability Gap. If an autonomous weapon kills the wrong person, who goes to jail? We cannot put a computer program in prison, which creates a massive legal vacuum.
  To understand why this is a fatal flaw for lethal AI, we must look at the rules proposed by the Australian Government (D2). In their report, the government lists Accountability as the very first mandatory guardrail for high-risk AI systems. According to the D2 glossary, being accountable means that someone must be answerable for decisions and can potentially be punished or sanctioned. Furthermore, Guardrail 7 in the D2 document states that organizations must establish processes so that people negatively impacted by AI systems can challenge the outcomes or make a complaint.
  When we apply these government guardrails to a battlefield, lethal AI fails completely. If a robotic weapon targets a civilian building by mistake, the victims families have no way to "challenge the outcome" or file a complaint against the machine. Because a machine has no money to pay fines and cannot feel guilt in a prison cell, it cannot be truly sanctioned. Therefore, the AI breaks the government most basic rule of accountability.
  Some people might argue that if the robot makes a mistake, we should just punish the programmer who built it. However, the researchers in Document D1 (Burton et al.) explain why this is extremely difficult. D1 points out that modern AI systems are highly complex, can change their own behavior through learning, and reason in ways that are very different from humans. Because AI can learn and adapt on its own, its final behavior can be very difficult for a human to predict.
  If an AI weapon learns a new, dangerous behavior on the battlefield, is it legally fair to punish a software developer who wrote the original code years ago in a laboratory? It is very hard to prove that the developer intentionally wanted the machine to commit a war crime. As Burton et al. note, the behavior of these systems can be incredibly hard to monitor or explain.
  Ultimately, this Accountability Gap makes Lethal Autonomous Weapons Systems too dangerous to use. If we cannot punish the robot, and we cannot fairly blame the developer who built the unpredictable machine, then no one is held responsible. By the Australian Government's own definitions, an AI system that lacks clear accountability should never be permitted in a high-risk setting.
