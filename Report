Section 1: Introduction
  AI is slowly replaced human in some daily tasks nowadays. From housework, transportation and even making decision of someone's life. It is more concerning now that technologies are capable of choosing and engaging targets without human control. Because of that, many questions are raised about this problem, but the most asked question is whether AI systems should be permitted to exercise lethal force. Although AI is very convenient and brings many advantages for work, the rules or the mandatory guardrails for AI have not been finalized and failed to meet the safety requirements to be able to use it. The most important thing in these systems is not about how they are developed, but a profound ethical challenge. Eventhough AI is every smart and fast, it still lacks of human emotion, empathy and moral judgment. This report will argue that weapons run on its own should not be allowed to exercise lethal force. To understand why these systems are too dangerous, we should take a clearer look at the ethical responsibility of the people who build these robots and how the Australian government made these safety rules.

Section 2: An overview of documents D1 and D2
  To understand the danger of lethal AI we should look at how the technology is created in the first place. In D1, Burton and other researchers argue that ethical considerations in AI should not be the final check at the end of the development, they should be integrated into the very first stages of an AI development. Normally, developers only focus on how to make an AI work fast and accurately and they treat ethical safety as an afterthought. Burton argues this is a huge mistake because if we want the machine or the AI to be safe to use, developers need to be taught to integrate ethics in the very first steps of the development. Therefore, they suggest that the responsibility for the machine's behavior is strictly linked to the building process. If not, it means that the engineer is not taught to prioritize ethical constrains, they might build a system that only focus on completing the mission, ignoring the value of a human life. If we build a lethal weapon that runs independently without proactive ethics, the product has already failed right in the lab before it even reaches the battlefield. Therefore, the people coding these weapons must carry some of the moral weight for what the machine eventually does. Moving to Document D2, while the Burton and researchers focus on the builders in D1, goverments must follow the laws. The Australian Government has recently addressed this in a 2024 proposals paper about safe and responsible AI. They see that AI can be useful but can also be very dangerous if not treat properlly. To prevent this from harming humans, the Australian Government is proposing "mandatory guardrails". These are strict, unbreakable safety rules for any AI used in such dangerous areas. A "high-risk setting" is defined as any situation where AI could hurt someone's physical safety, mental health, or legal rights. A weapon designed to kill people is the ultimate high-risk setting. According to the government, lethal AI must follow specific rules. First, there must be accountability, it means that it is clear who is legally responsible if things go wrong. Second, there must be transparency, it means people can understand how the AI made its decision. Finally, there must be human control mean a person must be able to oversee and step in to stop the machine. When we combine the ethical warnings from developers (D1) with the strict legal safety rules from the government (D2), a clear problem emerges. Lethal Autonomous Weapons Systems are designed to operate without human control. By their very nature, they break the mandatory guardrails that the Australian Government says are necessary for public safety. To prove that AI should not be allowed to kill, the rest of this report will explore three key problems in detail. First, it will examine the Accountability Gap, looking at why it is legally impossible to punish a machine if it commits a war crime. Second, it will explore the Transparency Problem, explaining why modern AI is often a "black box" that even its creators cannot fully understand. Finally, it will discuss the Failure of Meaningful Human Control, showing that when machines move at lightning speed, human oversight becomes nothing more than an illusion.

Section 3: The Accountability Gap
  In traditional warfare, there is a clear chain of command. If a soldier makes a terrible mistake and kills an innocent civilian, that soldier—or their commanding officer—can be put on trial and sent to prison. This is how international law works, a human is always held responsible. However, when we replace human soldiers with Artificial Intelligence (AI), this clear line of responsibility disappears. This problem is known as the Accountability Gap. If an autonomous weapon kills the wrong person, who goes to jail? We cannot put a computer program in prison, which creates a massive legal vacuum. To understand why this is a fatal flaw for lethal AI, we must look at the rules proposed by the Australian Government (D2). In their report, the government lists Accountability as the very first mandatory guardrail for high-risk AI systems. According to the D2 glossary, being accountable means that someone must be answerable for decisions and can potentially be punished or sanctioned. Furthermore, Guardrail 7 in the D2 document states that organizations must establish processes so that people negatively impacted by AI systems can challenge the outcomes or make a complaint. When we apply these government guardrails to a battlefield, lethal AI fails completely. If a robotic weapon targets a civilian building by mistake, the victims families have no way to "challenge the outcome" or file a complaint against the machine. Because a machine has no money to pay fines and cannot feel guilt in a prison cell, it cannot be truly sanctioned. Therefore, the AI breaks the government most basic rule of accountability. Some people might argue that if the robot makes a mistake, we should just punish the programmer who built it. However, the researchers in Document D1 (Burton et al.) explain why this is extremely difficult. D1 points out that modern AI systems are highly complex, can change their own behavior through learning, and reason in ways that are very different from humans. Because AI can learn and adapt on its own, its final behavior can be very difficult for a human to predict. If an AI weapon learns a new, dangerous behavior on the battlefield, is it legally fair to punish a software developer who wrote the original code years ago in a laboratory? It is very hard to prove that the developer intentionally wanted the machine to commit a war crime. As Burton et al. note, the behavior of these systems can be incredibly hard to monitor or explain. Ultimately, this Accountability Gap makes Lethal Autonomous Weapons Systems too dangerous to use. If we cannot punish the robot, and we cannot fairly blame the developer who built the unpredictable machine, then no one is held responsible. By the Australian Government's own definitions, an AI system that lacks clear accountability should never be permitted in a high-risk setting.

Section 4: Transparency and Black Box Problem 
  Another huge reason AI should never be allowed to kill is the Transparency Problem. In tech, this is often referred to as the black box problem. Modern AI systems work on very complex code. As a result, as the Australian Government (D2) points out, sophisticated AI models rely on data that may be just too large and complex for humans to process easily. The ways in which these systems reason are multi-level, so the explanation of how they actually arrive at a result is extremely limited. Of course, the decisions made by these AI systems are not always traceable. That means we can’t just look inside the computer to figure out why it decided to do something. Why a weapon is fired, and in a war with such a weapon of sorts in hands, is a matter of life or death. Did the robot trigger the shot after recognizing the enemy soldier carrying a weapon correctly? Or did it shoot because it faltered under the shadow? AIs are incredibly complex and reason in processes that diverge significantly from conventional human thought, Burton and the researchers say. In addition, they can change their own behaviors with time. For example, AIs based on personal data will analyze historical events to uncover patterns of human behavior. If a lethal weapon is able to change how it thinks all the time, and we can’t predict (or account for) its new behavior, then it is too unpredictable to be trustworthy. Such a misapprehension is in conflict with the Australian Government's safety rules. In Document D2, the government mentions that we need transparency to determine whether harm has transpired and, if so, how it can be anticipated (and predicted), future damages would be affected by harm. The government developed Guardrail 8 to address this question, mandating organizations to be open about AI models and systems. This regulation states that the creators of the AI should share data about what their AI is and isn't capable of, and the risks to its use such that everyone using their AI can understand what it can and cannot do. And Guardrail 9 requires companies, too, to keep detailed records-like methodologies for testing and design specs-that other parties can look after to see if they are following the rules. But if the technology itself is a black box, then it is physically impossible to comply effectively with these transparency provisions. If the drone's very own artificial brain makes decisions that can't be traced, a military commander cannot adequately gauge the constraints or hazards of a lethal drone. Even with perfect record-keeping, the internal reasoning of the machine remains an enigma. If a commander does not have reasonable grounds to believe the AI decided to attack a certain target, they cannot say that the attack was legal or safe. And because autonomous weapons do not offer human explanations for their actions, they fail the very strict transparency tests we apply to risky systems. To grant them the ability to kill would be to accept a weapon that works for goals we may not ever understand.

Section 5: The Failure of Meaningful Human Control 
  Even if we were allowed to somehow solve the accountability and transparency issues, fatal AI will face one last (as always, impossible) hurdle to human control. There's always someone waiting in the movies who is always poised to push a red button to bring a robot to a halt if it's got a bad time. For the real world, however, military AI moves a lot faster than the human brain can. This means that a human technically may be “in the loop” – but they don’t actually have any power to stop a bad decision. This entirely violates the safety guidelines necessary to maintain the safety of high-risk AI. The Australian Government specifically considers this as the problem in Document D2 with Guardrail 5 - the requirement that organisational mechanisms will require a human or intervene human as a means of creating a meaningful degree of human control. The document explains that organizations need to ensure humans can efficiently analyze the system and monitor its functioning, with action taken to interrupt it if deemed necessary. The government acknowledges that human intervention in real time isn’t always feasible, but it urges developers to put the system in such a way that there is a human who can review outputs and overturn decisions if necessary. Apply Guardrail 5 to Lethal Autonomous Weapons Systems, and the rule comes crashing down. An AI drone operating across a battlefield is parsing thousands of pieces of data in one second. AI systems have the ability to analyze massive amounts of data, identify behaviors, and choose actions often much faster and better than humans (Burton, Document D1). If the machine is making these decisions at machine speed, there is no way to review it with a human operator to reverse a decision taken before the machine's weapon fires. Document D1 also warns us that these quick, high-speed choices can be deadly. The researchers also point out that AI behavior is extremely hard for humans to closely monitor as it makes these extremely swift decisions. They provide a real-life example of high-speed algorithmic stock trading, which has triggered “flash crashes” in the economy: The computers moved faster than humans were able to pull the plug. This is strengthened in Santoni de Sio & van den Hoven (2018) that the idea of a human 'in the loop' is of no value, when that human does not have the time or ability to decipher the logic of the AI as before an intervention takes place. If an AI can cause a crash in the stock market before someone can intervene, it can indeed blast a missile before humans can block it. This makes any human oversight of a potentially lethal autonomous weapon, essentially an illusion. If a soldier who watches a screen has half a second to determine whether the AI’s target is a perceived reality threat or an error, that soldier will probably just put his trust in the machine and let it happen. This type of control is not “meaningful” control. People can't keep pace with intelligent AI, so we cannot have it monitored and controlled in safety. If we don’t have control over it, then we can’t legally roll it out under the very strict guardrails proposed by the Australian Government.

Section 6: Do Humans Have an Advantage over Robots? 
  Some people say that Artificial Intelligence in war is really a good idea. Robots that kill advocates say some wars are just (and necessary). The use of killer robots, they also contend, will help to drive human soldiers from the line of fighting to keep them free. So, as Arkin (2009) has said in these discussions, the contention is that robots would really prove better than humans at obeying laws of war and rules of engagement. Because machines don’t experience fear, anger, or desire for revenge, they are less likely to make emotional mistakes on the battlefield. But examine Document D1, and see this argument as utterly misguided. D1 teaches us about the “virtue ethics,” a way of thinking in which the question: whom, then, should I be? Phronesis, or practical wisdom, is an important element of living well in this theory. It is the capacity of the human being to assess a given mess and how one ought to react appropriately to a particular mess. Robots may be great at simple math rules because they know better -- but they don't know this practical wisdom. In a war zone, a soldier must have practical intelligence to distinguish between an enemy fighter and a scared civilian who is gripping a camera. A robot without “phronesis” cannot comprehend this human context. Moreover, even if a robot were technically “better” at shooting targets, it would still violate the legislation established by the Australian Government (Document D2). The state expressly emphasizes that organizations have procedures in place for people affected by AIs to contest use or results. If, say, an AI makes a decision using numbers instead of human knowledge and it makes a horrible mistake, there is no way an AI victim can properly challenge that machine’s decision. So it is inevitable that there will be no human thought and legal accountability; the "perfect robot" argument ultimately will collapse.

Section 7: Conclusion
  Allowing AI systems to decide everything in a given fraction of a second gives them enormous access to our lives. And although that may be perfectly okay with simple software, it is catastrophic when it comes to Lethal Autonomous Weapons Systems. The key issue is whether these AI systems should be permitted to kill and, based on current ethical and even legal frameworks, the answer is no. As Document D1 demonstrates, the behavior of AI systems can be extremely hard to verify, predict or explain, because an AI reasons completely unlike human thinking. When the developers themselves do not always know what the AI is going to do, it represents a failure of “proactive ethics” to render a machine combatant. We cannot, from the inside, build systems that are ethical if we do not understand, not even to theory, how they ultimately reach their decisions. Further, Document D2 of the Australian Government’s proposals indicates that high-risk AI needs to be tightly controlled. The state needs clear accountability, human oversight and transparency. Autonomous weapons are “black boxes” that move at breakneck speed and therefore circumvent all these mandatory guardrails. They engender an accountability gap, in which no one gets punished, and a control gap in which humans can’t act in time. Thus, until the technology is able to achieve 100% transparency and substantial human regulation, AI should never be allowed to engage as a mere tool of lethal force. We do need, however, that the ultimate choice to take a human life always be hands-on.
