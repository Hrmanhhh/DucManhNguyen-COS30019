  AI is slowly replaced human in some daily tasks nowadays. From houseworks, transportation and even making decision of someone's life. It is more concerning now that technologies are capable of choosing and engaging targets without human control. Because of that, many questions are raised about this problem, but the most asked question is whether AI systems should be permitted to exercise lethal force. Although AI is very convenient and brings many advantages for work, the rules or the mandatory guardrails for AI have not been finalized and failed to meet the safety requirements to be able to use it.
  The most important thing in these systems is not about how they are developed, but a profound ethical challenge. As AI is being asked to decide those risky real world applications, making the boundaries of legal and moral responsibility become blurred. As has been said, we should take a clearer look at how researchers suggested in D1 document and how the Australian government deal with this situation in D2 document.
  To start with, in D1 the authors argue that ethical considerations in AI should not be the final check at the end of the development, they should be integrated into the very first stages of an AI developments. They expect developers to be trained to foresee the ethical consequences in their code. Therefore, they suggest that the responsibility for the machine's behavior is strictly linked to the building process. If not, it means that the engineer is not taught to prioritize ethical constrains making the development of the robot or the AI fail right in the lab before being used in a certain area.
  Moving to Document D2, this is about the law made by the Australian Government. In 2024, all AI used in dangerous areas, like hurting people or making unwanted decisions, will have to follow the Mandatory Guardrails as a way to keep it safe to use. D2 says we must have three things and these are accountability, transparency and human oversight.
  Looking at the reality, we can see that the AI can not pass these tests. When it is deployed, it works on its own. This means it is harder for humans to oversee it, and the conflict between the Australian Government's Mandatory Guardrail and how the technology works remains.
  The most important problem is the Accountability Gap. We can see in a normal war that if a soldier kills an innocent citizen, that soldier can go to jail or even be executed. But this can't be applied to AI because if the AI does it, who takes the blame, the AI or the engineer? We cannot take AI to prison and as D1 points out, it is hard to blame the programmer as well.
